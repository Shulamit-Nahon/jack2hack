new_tokeniser := proc(in_file, out_file) is
	local tokeniser := [
		keywords = {
			'class',
			'constructor',
			'function',
			'method',
			'field',
			'static',
			'var',
			'int',
			'char',
			'boolean',
			'void',
			'true',
			'false',
			'null',
			'this',
			'let',
			'do',
			'if',
			'else',
			'while',
			'return'
		},
		symbols = {},
		fin = io.open(in_file, r),
		fout = io.open(out_file, w)
	]

	proc tokeniser@@tokenise(_) is
		#-- open tokeniser
		io.write(self.fout, '<tokens>\n')
		local curr_char := self.fin.read()
		while(next_char := self.fin.read()) do
			io.write(self.fout,'\t<',tt,'>',tv,'</',tt,'>\n')
			curr_char := next_char
		od
		#-- close tokeniser
		io.write(self.fin, '</tokens>')
		io.close(self.fout)
		io.close(self.fin)
	end

	proc tokeniser@@read_token(_) is
		local current_input := io.read(in_file, 1)
		local next_input := io.read(in_file, 1)

	end

	return tokeniser
end