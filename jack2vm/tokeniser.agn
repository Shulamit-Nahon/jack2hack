run(jack2vm_dir & '/../lib/xml_encode')
run(jack2vm_dir & '/../lib/escape_whitespace')

new_tokeniser := proc(in_file) is
	local tokeniser := [
		keywords = {'class', 'constructor', 'function', 'method', 'field', 'static', 'var', 'int', 'char', 'boolean', 'void', 'true', 'false', 'null', 'this', 'let', 'do', 'if', 'else', 'while', 'return'},
		symbols = {'{', '}', '(', ')', '[', ']', '.', ',', ';', '+', '-', '*', '/', '&', '|', '<', '>', '=', '~'},
		fin = io.open(in_file, 'r'),
		curr_char,
		next_char,
		tokens = []
	]

	proc tokeniser@@tokenise(_) is
		local token := ''
		#-- open tokens tag
		self.curr_char := io.read(self.fin, 1)
		self.next_char := io.read(self.fin, 1)
		while self.curr_char <> null do

			#-- if char is a one line comment
			if self.curr_char = '/' and self.next_char = '/' then
				#-- till end of line
				token := self@@read_while(<< (c) -> c <> '\n' >>)
				self@@discard(1) #-- '\n'
			elif self.curr_char = '/' and self.next_char = '*' then
				token := self@@read_while(<< (c1, c2) -> c1 <> '*' or c2 <> '/' >>)
				self@@discard(2) #-- '*/'

			#-- if char is a symbol
			elif self.curr_char in self.symbols then
				token := self@@read_while(<< (c) -> c in self.symbols >>)
				for sym in token do
					self@@add_token('symbol', sym)
				od

			#-- if char is the start of a string
			elif self.curr_char = '"' then
				self@@discard(1) #-- '"'
				token := escape_whitespace(self@@read_while(<< (c) -> c <> '"' >>))
				self@@discard(1) #-- '"'
				#-- discard the first character of token which is an open quote
				self@@add_token('stringConstant', token)

			#-- if char is digit
			elif strings.isnumber(self.curr_char) then
				token := self@@read_while(<< (c) -> strings.isnumber(c) >>)
				self@@add_token('integerConstant', token)

			#-- if char is beginning of identifier or keyword (aka word)
			elif strings.isalpha(self.curr_char) or self.curr_char = '_' then
				token := self@@read_while(<< (c) -> strings.isalphanumeric(c) or c = '_' >>)
				#--check to see if identifier is keyword
				if token in self.keywords then
					self@@add_token('keyword', token)
				else
					self@@add_token('identifier', token)
				fi

			#-- if its whitespace, skip
			elif self.curr_char in {' ', '\t', '\n', '\r'} then
				self@@discard(1)

			else
				error('Unexpected character "' & self.curr_char & '"')
			fi
		od
		#-- close tokeniser
		io.close(self.fin)
		return self.tokens
	end

	#-- returns a token formed of all the consecutive next characters for which predicate(self.curr_char, self.next_char) is true
	proc tokeniser@@read_while(predicate) is
		local token := ''
		while predicate(self.curr_char, self.next_char) do
			token &:= self.curr_char
			self@@discard(1)
		od
		return token
	end

	#-- will discard the next n characters (default is 1)
	proc tokeniser@@discard(n) is
		n := n or 1
		for i from 1 to n do
			self.curr_char := self.next_char
			self.next_char := io.read(self.fin, 1)
		od
	end

	proc tokeniser@@add_token(kind, value) is
		insert new_token(kind, value) into self.tokens
	end

	return tokeniser
end