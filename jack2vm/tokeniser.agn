new_tokeniser := proc(in_file, out_file) is
	local tokeniser := [
		keywords = {
			'class',
			'constructor',
			'function',
			'method',
			'field',
			'static',
			'var',
			'int',
			'char',
			'boolean',
			'void',
			'true',
			'false',
			'null',
			'this',
			'let',
			'do',
			'if',
			'else',
			'while',
			'return'
		},
		symbols = {
			'{',
			'}',
			'(',
			')',
			'[',
			']',
			'.',
			',',
			';',
			'+',
			'-',
			'*',
			'/',
			'&',
			'|',
			'<',
			'>',
			'=',
			'~'
		},
		fin = io.open(in_file, 'r'),
		fout = io.open(out_file, 'w')
	]

	proc tokeniser@@tokenise(_) is
		local token, curr_char
		#-- open tokens tag
		io.writeline(self.fout, '<tokens>')
		local curr_char := io.read(self.fin)
		while (next_char := io.read(self.fin)) do
			#-- if char is comment
			if (curr_char = '/') then
				#-- till end of line
				if (next_char = '/') then
					do
						next_char := io.read(self.fin)
						if (next_char = '\n') then
							break
						fi
					od
				elif (next_char = '*') then
					do
						next_char := io.read(self.fin)
						if (curr_char = '*' and next_char = '/') then
							next_char := io.read(self.fin)
							break
						fi
						curr_char := next_char
					od
				fi

			#-- if char is a symbol
			elif (curr_char in symbols) then
				self@@out('symbol', curr_char)

			#-- if char is a string
			elif (curr_char = '"') then
				#-- create local string
				local str := ''
				#-- if immediate end of string, break
				if (next_char = '"') then
					break
				fi
				str &:= next_char
				while (next_char := io.read(self.fin)) do
					if (next_char = '"') then
						break
					fi
					str &:= next_char
				od
				self@@out('stringConstant', str)

			#-- if char is digit
			elif (curr_char in digits) then
				token, curr_char := self@@read_int(curr_char)
				self@@out('intConstant', token)

			#-- if char is beginning of identifier or keyword (aka word)
			elif (strings.isalpha(curr_char) or curr_char = '_') then
				token, curr_char := self@@read_word(curr_char)
				#--check to see if identifier is keyword
				if (token in keywords) then
					self@@out('keyword', token)
				else
					self@@out('identifier', token)
				fi
			fi
		od
		#-- close tokeniser
		io.write(self.fout, '</tokens>')
		io.close(self.fout)
		io.close(self.fin)
	end

	proc tokeniser@@read_int(curr_char) is
		local num := ''
		while (strings.isnumber(curr_char)) do
			num &:= curr_char
			curr_char := io.read(self.fin)
		od
		return num, curr_char
	end

	proc tokeniser@@read_word(curr_char) is
		local word := ''
		while (strings.isalphanumeric(curr_char)) do
			word &:= curr_char
			curr_char := io.read(self.fin)
		od
		return word, curr_char
	end

	proc tokeniser@@out(tag, value) is
		io.writeline(self.fout, '\t<' & tag & '> ' & value & ' </' & tag & '>')
	end

	return tokeniser
end